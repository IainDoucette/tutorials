{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to instantiating models with EUGENe\n",
    "Adam Klie (last updated: *09/20/2023*)\n",
    "***\n",
    "**Description:**\n",
    "This notebook is meant to serve as a guide to instantiating models with EUGENe. It will cover the following topics:\n",
    "1. EUGENe's `models` library\n",
    "2. Using EUGENe's built-in models\n",
    "3. Using configuration files to instantiate models\n",
    "4. Using custom architectures\n",
    "5. Using custom LightningModules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to have [installed EUGENe](https://eugene-tools.readthedocs.io/en/latest/installation.html) in your environment before you start!\n",
    "> **Warning**:\n",
    "> Before you start! Running this notebook without a GPU on this data is feasible but will be very slow. We'd recommend using [Google Colab](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) if you don't have access to your own GPU. If you choose this option, run the following before you begin the tutorial:\n",
    "> ```\n",
    "> !pip install eugene-tools\n",
    "> !pip install torchmetrics==0.1a1.4\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out basic torch\n",
    "x_linear = torch.randn(10, 4)\n",
    "x = torch.randn(10, 4, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a few cells near the end of the notebook that illustrate plugging in EUGENe architectures into different software frameworks, you will need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EvoAug set-up\n",
    "#!pip install evoaug\n",
    "#!pip install git+https://github.com/p-koo/evoaug_analysis.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend cloning the entire tutorials repository so that you have all the necessary intermediate files you need, but when applicable, we also provide links to download the files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to where you would like to save all your results, likely your tutorials download directory\n",
    "import os\n",
    "os.chdir(\"/cellar/users/aklie/projects/ML4GLand/tutorials\")  # TODO: change this to your own directory\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EUGENe's `models` library -- the basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to think of architecting neural networks as playing with adult legos. Legos specifically designed for geeks who like to learn from data. \n",
    "\n",
    "Designing and training neural networks for regulatory genomics requires a comprehensive library of architecture lego blocks. Fortunately for us, PyTorch provides an extensive library that we can use right out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a simple linear layer with 4 inputs and 5 outputs.\n",
    "layer = nn.Linear(4, 5)\n",
    "layer_out = layer(x_linear)\n",
    "layer, layer_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUGENe's layers build on PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EUGENe builds on PyTorch by adding several useful layers such as inception and residual layers. These are define in the `eugene.models.base._layers` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EUGENe's layers \n",
    "from eugene.models.base import _layers as layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers in EUGENe can be broken up into several categories: **Activations, Convolutional, Pooling, Recurrent, Attention, Normalizer, Wrappers, Gluers, Sampling, Noise, Misc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take exponential activations as an example that have been shown to improve the interpretability of models\n",
    "layer = layers.Exponential(inplace=False)\n",
    "layer_out = layer(x)\n",
    "layer, layer_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more complex example, we can use the InceptionConv1D layer that uses multiple sizes of convolutions in a single layer\n",
    "layer = layers.InceptionConv1D(in_channels=4, out_channels=16)\n",
    "layer_out = layer(x)\n",
    "layer, layer_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the MultiHeadAttention layer that is used in the Transformer architecture\n",
    "layer = layers.MultiHeadAttention(\n",
    "    input_dim=4,\n",
    "    head_dim=10,\n",
    "    num_heads=2\n",
    ")\n",
    "layer_out = layer(x.transpose(1, 2), mask=None)\n",
    "layer, layer_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember you can always bring up the signature and docstring for any layer by using the `?` operator in Jupyter notebooks. For example, to bring up the docstring for the `Inception` layer, you can run the following code:\n",
    "\n",
    "```python\n",
    "from eugene.models.base._layers import InceptionConv1d\n",
    "InceptionConv1d?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more common example of what we call a wrapper layer. Residual layers are used to add the input to the output of a layer.\n",
    "conv_layer = torch.nn.Conv1d(4, 4, 5, padding=\"same\")\n",
    "layer = layers.Residual(conv_layer)\n",
    "layer_out = layer(x)\n",
    "layer, layer_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From layers to blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, EUGENe introduces flexible functions for establishing common “blocks” that are composed of heterogeneous sets of layers arranged in a predefined or adaptable order. Blocks are available in the `eugene.models.base._blocks` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base import _blocks as blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional block (Conv1DBlock in EUGENe) often comprises convolutional, normalization, activation, and dropout layers in different orderings depending on the model and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_block = blocks.Conv1DBlock(\n",
    "    input_len=100,\n",
    "    input_channels=4,\n",
    "    output_channels=32,\n",
    "    conv_kernel=23,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "block_out = conv1d_block(x)\n",
    "conv1d_block, block_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Order matters! From model to model, the order of the layers can be employed differently. For example, a DeepSEA conv block uses the ordering “conv-act-pool-dropout”, while a DeepSTARR conv block uses “conv-norm-act-pool”. We can change the ordering of the layers in the conv block using the order argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A la DeepSEA, note that we can also omit one of the layers by simply not including it in the order argument. We've also added other arguments that are flexible in EUGENe\n",
    "conv1d_block = blocks.Conv1DBlock(\n",
    "    input_len=100,\n",
    "    input_channels=4,\n",
    "    output_channels=32,\n",
    "    conv_kernel=23,\n",
    "    conv_type=\"conv1d\",\n",
    "    conv_padding=\"same\",\n",
    "    pool_type=\"max\",\n",
    "    norm_type=\"batchnorm\",\n",
    "    dropout_rate=0.5,\n",
    "    order=\"conv-act-pool-dropout\"\n",
    ")\n",
    "block_out = conv1d_block(x)\n",
    "conv1d_block, block_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A la DeepSTARR\n",
    "conv1d_block = blocks.Conv1DBlock(\n",
    "    input_len=100,\n",
    "    input_channels=4,\n",
    "    output_channels=32,\n",
    "    conv_kernel=23,\n",
    "    conv_type=\"conv1d\",\n",
    "    conv_padding=\"same\",\n",
    "    pool_type=\"max\",\n",
    "    norm_type=\"batchnorm\",\n",
    "    dropout_rate=0.5,\n",
    "    order=\"conv-norm-act-pool\"\n",
    ")\n",
    "block_out = conv1d_block(x)\n",
    "conv1d_block, block_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also offer 2 other blocks: `DenseBlocks`, `RecurrentBlocks`. We leave it as an exercise to the reader to explore these blocks or to create their own! (Transformer blocks coming soon!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From blocks to towers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also stack multiple blocks on top of one another so that we can build very deep architectures without having to write a lot of code. We call these stacks of blocks \"towers\". Towers are available in the `eugene.models.base._towers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base import _towers as towers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a single flexible `Tower` class that can handle any block. The class takes in a set of `static_block_args` that you want to repeat across blocks and a set of `dynamic_block_args` that you want to change for each block. You can also pass in a set of `mults` that you want to use to scale an argument with in each block (e.g. if you want to do exponentially increasing dilations like in Basenji). We show an example of using the `Tower` class with a Conv1DBlock (probably the most common in genomics) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base import _blocks as blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we toss a Conv1DBlock with some different arguments than before. If you like dogs, then this tower is for you.\n",
    "tower = towers.Tower(\n",
    "    input_size=(4, 100),\n",
    "    block=blocks.Conv1DBlock,\n",
    "    repeats=3,\n",
    "    static_block_args={'input_len': 100, 'conv_kernel': 3, 'conv_padding': 'same', 'conv_type': 'conv1d', 'activation': 'gelu', 'order': 'conv-norm-act-dropout-pool'},\n",
    "    dynamic_block_args={'input_channels': [4, 10, 20], 'output_channels': [10, 20, 30]},\n",
    "    mults={\"conv_dilation\": 2}\n",
    ")\n",
    "tower_out = tower(x)\n",
    "tower, tower.input_size, tower.output_size, tower_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other cool thing about this is it is not EUGENe specific! That is, it isn't constrained to working on EUGENe layers or blocks. You can use it with any PyTorch layer or block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just repeat a linear layer 3 times, decreasing the number of units in each layer\n",
    "tower = towers.Tower(\n",
    "    input_size=400,\n",
    "    block=torch.nn.Linear,\n",
    "    repeats=3,\n",
    "    dynamic_block_args={'in_features': [400, 200, 100], 'out_features': [200, 100, 10]},\n",
    ")\n",
    "tower_out = tower(x.reshape(10, -1))\n",
    "tower, tower.input_size, tower.output_size, tower_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using standarized objects such as blocks and towers has several benefits:\n",
    "1. It makes it easier to write code, and for others to read your code.\n",
    "2. It makes it easier to interpret models. You can pull layers out of the model based on the standard names and see what is going on.\n",
    "3. It makes it easier to debug models and identify where things are going wrong."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EUGENe's built-in models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures as simple nn.Modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to architectures in EUGENe as any class that inherits from `torch.nn.Module` and includes a `forward` method. This is the standard way of defining architectures in PyTorch. This means that any of the layers, blocks, or towers we discussed are technically architectures on their own. However, architectures that are actually used in practice are usually composed of multiple layers, blocks, and towers in combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EverythingEverywhereAllAtOnce(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super(EverythingEverywhereAllAtOnce, self).__init__()\n",
    "        self.layer = nn.Linear(4, 5)\n",
    "        self.layer2 = layers.Exponential(inplace=False)\n",
    "        self.layer3 = layers.InceptionConv1D(in_channels=4, out_channels=16)\n",
    "        self.layer4 = layers.MultiHeadAttention(\n",
    "            input_dim=4,\n",
    "            head_dim=10,\n",
    "            num_heads=2\n",
    "        )\n",
    "        self.layer5 = layers.Residual(conv_layer)\n",
    "        self.layer6 = blocks.Conv1DBlock(\n",
    "            input_len=100,\n",
    "            input_channels=4,\n",
    "            output_channels=32,\n",
    "            conv_kernel=23,\n",
    "            dropout_rate=0.1,\n",
    "        )\n",
    "        self.layer7 = towers.Tower(\n",
    "            input_size=(4, 100),\n",
    "            block=blocks.Conv1DBlock,\n",
    "            repeats=3,\n",
    "            static_block_args={'input_len': 100, 'conv_kernel': 3, 'conv_padding': 'same', 'conv_type': 'conv1d', 'activation': 'gelu', 'order': 'conv-norm-act-dropout-pool'},\n",
    "            dynamic_block_args={'input_channels': [4, 10, 20], 'output_channels': [10, 20, 30]},\n",
    "            mults={\"conv_dilation\": 2}\n",
    "        )\n",
    "        self.layer8 = towers.Tower(\n",
    "            input_size=400,\n",
    "            block=torch.nn.Linear,\n",
    "            repeats=3,\n",
    "            dynamic_block_args={'in_features': [400, 200, 100], 'out_features': [200, 100, 10]},\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, the forward for this architecture will not actually work, I just wanted to stack everything we've looked at so far into one model\n",
    "EverythingEverywhereAllAtOnce()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just leave you on your own to play with your new lego sets, but for those who want some fully assembled models to play with, we have you covered. We've built a model zoo (cliche I know) of models that are available in the `eugene.models.zoo` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models import zoo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we mentioned before, every architecture in the zoo is a class that inherits from `torch.nn.Module` and includes a `forward` method. This means that you can use any of the architectures in the zoo just like you would any other PyTorch model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of built-in architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ever the [splitters at EUGENe](https://en.wikipedia.org/wiki/Lumpers_and_splitters), so we split our models into many categories based on the types of tasks they were designed for. The model zoo has the following sections:\n",
    "\n",
    "1. Basic models\n",
    "2. Transcription factor binding prediction models\n",
    "3. Regulatory classification models\n",
    "4. Cis-regulatory element (CRE) activity prediction models\n",
    "5. Profile predictors\n",
    "6. Single cell predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking out the basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.zoo._basic_models import FCN, CNN, RNN, Hybrid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide customizable fully connected (FCN), convolutional (CNN), recurrent (RNN) and hybrid (a combination of the three) architectures that can all be instantiated from single function calls! Did I say we were splitters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNs are just a bunch of linear layers stacked on top of each other\n",
    "model = FCN(\n",
    "    input_len=100,\n",
    "    output_dim=10,\n",
    "    dense_kwargs={\n",
    "        \"hidden_dims\": [50, 25],\n",
    "        \"activations\": \"relu\",\n",
    "        \"batchnorm\": True,\n",
    "    }\n",
    ")\n",
    "model_out = model(x)\n",
    "model, model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is just another example of an FCN, this time with a different activation function and dropout, as well as other customizable parameters\n",
    "model = FCN(\n",
    "    input_len=100,\n",
    "    output_dim=1,\n",
    "    dense_kwargs=dict(\n",
    "        hidden_dims=[100, 50, 25], \n",
    "        activations=[\"relu\", None, None], \n",
    "        dropout_rates=[0.1, 0.5], \n",
    "        batchnorm=True, \n",
    "        batchnorm_first=True, \n",
    "        biases=False\n",
    "    ),\n",
    ")\n",
    "model_out = model(x)\n",
    "model, model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move to CNNs, which are just a bunch of convolutions stacked on top of each other, followed by a dense block\n",
    "model = CNN(\n",
    "    input_len=100,\n",
    "    output_dim=10,\n",
    "    conv_kwargs={\n",
    "        \"input_channels\": 4,\n",
    "        \"conv_channels\": [10, 10],\n",
    "        \"conv_kernels\": [5, 3],\n",
    "        \"activations\": [],\n",
    "        \"pool_types\": []\n",
    "    }\n",
    ")\n",
    "model_out = model(x)\n",
    "model, model_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of incoming features to the dense block is determined automagically! Thanks [torchinfo](https://github.com/TylerYep/torchinfo)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's skip RNNs since those aren't used a whole lot and jump to Hybrids, which are just convolutions followed by recurrent layers and then a dense block\n",
    "model = Hybrid(\n",
    "    input_len=100,\n",
    "    output_dim=10,\n",
    "    conv_kwargs={\n",
    "        \"input_channels\": 4,\n",
    "        \"conv_channels\": [10, 10],\n",
    "        \"conv_kernels\": [5, 3],\n",
    "        \"activations\": \"relu\",\n",
    "        \"pool_types\": \"max\"\n",
    "    },\n",
    "    recurrent_kwargs={\n",
    "        \"hidden_dim\": 10,\n",
    "        \"num_layers\": 10,\n",
    "        \"bidirectional\": True\n",
    "    }\n",
    ")\n",
    "model_out = model(x)\n",
    "model, model_out.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage the user to play with different arguments to these architectures, and not just because we selfishly want them to help us find any bugs that have missed our testing ;)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Published architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, even having to specifiy the arguments to the layers you want is too much. We get it. We've been there. That's why we've built in set of models that are designed for specific tasks. We have also constructed several published architectures that often represent specific configurations of these basic architectures and made them accessible to users through single function calls. One example is the transcription factor binding classifiers. These models are available in the `eugene.models.zoo._tf_binding_predictors` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.zoo._tf_binding_predictors import DeepBind, Kopp21CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a minimum, each architecture expects an input sequence length (`input_len`) and an output dimension (`output_dim`). Other required and optional arguments are model specific, but each model follows the same contract as before. That is, each model is a class that inherits from `torch.nn.Module` and includes a `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepBind(\n",
    "    input_len=100,\n",
    "    output_dim=1,\n",
    "    mode=\"rbp\"\n",
    ")\n",
    "model_out = model(x)\n",
    "model, model_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it as an exercise to the reader to explore the other published architectures in the model zoo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModules for training architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you may be thinking to yourself if you've made it this far is, \"Wait, how do I make sure I train the DeepBind architecture as the creator intended\" (e.g. for binary classification). It is true, there is nothing in the way we've written any of the built-in architectures requiring you to train them in any particular way (again, maybe we are lumpers). Instead, we've written that contract into something called a LightningModule from PyTorch Lightning! Don't worry, you don't have to know anything about PyTorch Lightning to use these modules, just on how they are used in EUGENe. As a starting place, we can grab LightningModules directly from the `eugene.models` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each LightningModules is meant to do the following:\n",
    "\n",
    "1. define the types of architectures it can train\n",
    "2. standardize the way a user interacts with those architectures in EUGENe\n",
    "3. reduce boilerplate PyTorch and PyTorch Lighting code. \n",
    "\n",
    "For example, we implemented SequenceModule to expect an architecture that ingests a single tensor (usually one-hot encoded DNA sequences) as input and outputs a single tensor. The SequenceModule defines how this class of models should be trained (including the loss function and optimizer), what metrics should be reported, and how inference should be handled. As a result, any PyTorch model that follows this contract can be trained using SequenceModule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to create a model architecture that we can pass to the module. We kept it simple with DeepBind here\n",
    "arch = DeepBind(\n",
    "    input_len=100,\n",
    "    output_dim=1\n",
    ")\n",
    "arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we pass the architecture to the module, along with several arguments that we want to use to train the model\n",
    "module = models.SequenceModule(\n",
    "    arch=arch,\n",
    "    task=\"binary_classification\",\n",
    "    loss_fxn=\"bce\",\n",
    "    optimizer=\"adam\",\n",
    "    metric=\"auroc\",\n",
    "    metric_kwargs={\"task\": \"binary\", \"num_classes\": 1},\n",
    ")\n",
    "module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! We have a architecture wrapped in a SequenceModule that is now ready for training! Let's break down the arguments in the code above.\n",
    "1. `task` - the task we are training the model for. This is used to determine the loss function and metrics to use when we don't pass them in explicitly.\n",
    "2. `loss_fxn` - the loss function to use for training. If not provided, the loss function is determined based on the task.\n",
    "3. `optimizer` - the optimizer to use for training.\n",
    "4. `metric` - the metric to use for training. If not provided, the metric is determined based on the task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the `SequenceModule` is expecting an architecture that follows the contract we described above. DeepBind, as well as most of the other built-in architectures, follow this contract. If we pass in an architecture that doesn't follow this contract, however, we will likely get an error when we try to train the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also implemented a ProfileModule that handles BPNet style training, where the model has multiple output tensors (“heads”), can take in optional control inputs, and uses multiple loss functions. We only have one architecture that follows this contract as of right now (BPNet), but who knows maybe other architectures will follow this contract in the future.                                                                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.zoo import BPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = BPNet(\n",
    "    input_len=2114,\n",
    "    output_dim=1000,\n",
    "    n_outputs=2,\n",
    "    n_control_tracks=2, \n",
    "    trimming=(2114 - 1000) // 2,\n",
    "    name=\"BPNet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models import ProfileModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = ProfileModule(arch=arch)\n",
    "module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using config files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, hopefully you have a pretty good feel how architectures work in EUGENe and we can move into some nuances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be quite cumbersome to drag around the arguments you need to build a model to every place you need to instantiate it. It can also be quite cumbersome to have to remember all the arguments you need to build a model. That's why we've built in a config file system that allows you to save and load model configurations. Let's start with an arleady generated config for a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from eugene import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run the following to get the config. You will not need to do this if you are working in your cloned tutorials repo\n",
    "#!mkdir -p $cwd/configs\n",
    "#!wget https://github.com/ML4GLand/tutorials/blob/main/configs/simple_cnn.yaml -O $cwd/configs/simple_cnn.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make use of a EUGENe helper for loading from a config\n",
    "models.load_config(\"configs/simple_cnn.yaml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the config directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $cwd/configs/simple_cnn.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it looks a lot like the arguments we passed into the CNN architecture. It's really the same process either way, the config file just has a little more persistence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs work similarly for the published architectures. Let's load in a config for the DeepBind architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run the following to get the config, you will not need to do this if you are working in your cloned tutorials repo\n",
    "#!mkdir -p $cwd/configs\n",
    "#!wget https://github.com/ML4GLand/tutorials/blob/main/configs/deepbind.yaml -O $cwd/configs/deepbind.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.load_config(\"configs/deepbind.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $cwd/configs/deepbind.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that when you build a SequenceModule with a config, you must use a built-in architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you don't have to add the SequenceModule specific arguments to the config if you don't want to. You can specify just the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run the following to get the config, you will not need to do this if you are working in your cloned tutorials repo\n",
    "#!mkdir -p $cwd/configs\n",
    "#!wget https://github.com/ML4GLand/tutorials/blob/main/configs/deepbind_arch.yaml -O $cwd/configs/deepbind_arch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.load_config(\"configs/deepbind_arch.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $cwd/configs/deepbind_arch.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I've said it twice, all say it three times, architectures in EUGENe are just classes that inherit from `torch.nn.Module` and include a `forward` method. This means that you can grab PyTorch models from anywhere and use them in EUGENe. Let's build an new architecture from scratch and plug it into a SequenceModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could imagine building this a ton of different ways, but here is a simple example\n",
    "import torch.nn.functional as F\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "\n",
    "        # Set the attributes\n",
    "        self.input_len = 100\n",
    "        self.output_dim = 1\n",
    "\n",
    "        # Create the blocks\n",
    "        self.conv1 = nn.Conv1d(4, 30, 21)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.max_pool1d(x, x.size()[-1]).flatten(1, -1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can pass this to the module\n",
    "models.SequenceModule(\n",
    "    arch=SmallCNN(),\n",
    "    task=\"binary_classification\",\n",
    "    loss_fxn=\"bce\",\n",
    "    optimizer=\"adam\",\n",
    "    metric=\"auroc\",\n",
    "    metric_kwargs={\"task\": \"binary\"}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And away we go!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom LigthningModules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse process by plugging in a built-in architecture to a custom LightningModule. Let's use the EvoAug RobustModel LightningModule, and plug in the DeepBind architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from evoaug import evoaug\n",
    "from evoaug_analysis import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the deepbind model that was trained in the evo_aug paper\n",
    "deepbind = DeepBind(249, 2)\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer_dict = utils.configure_optimizer(deepbind, lr=0.001, weight_decay=1e-6, decay_factor=0.1, patience=5, monitor='val_loss')\n",
    "model = evoaug.RobustModel(\n",
    "    deepbind, \n",
    "    criterion=loss, \n",
    "    optimizer=optimizer_dict, \n",
    "    augment_list=[]\n",
    ")\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, away we go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
