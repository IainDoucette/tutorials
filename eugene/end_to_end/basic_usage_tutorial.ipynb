{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage Tutorial\n",
    "Adam Klie (last updated: *09/20/2023*)<br>\n",
    "\n",
    "In this tutorial, we illustrate a basic end-to-end EUGENe workflow that trains and interprets a single task regression model on a [published dataset of plant promoters](https://www.nature.com/articles/s41477-021-00932-y). If you are just starting out with EUGENE, you are in the right place! This tutorial will walk you through the steps of preparing the data, training a single task regression model, and interpreting that trained model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to have [installed EUGENe](https://eugene-tools.readthedocs.io/en/latest/installation.html) in your environment before you start!\n",
    "> **Warning**:\n",
    "> Before you start! Running this notebook without a GPU on this data is feasible but will be very slow. We'd recommend using [Google Colab](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) if you don't have access to your own GPU.\n",
    "\n",
    "> **Note**:\n",
    "> We've noticed that for some IDE configurations, plots do not render in a Jupyter notebook unless you include the `%matplotlib inline` magic command. If you are having trouble rendering plots, make sure you have this line in your notebook or use `plt.show()` after each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the sometimes painful process of keeping track of global parameters and input/output file paths easier, we usually like to set these through EUGENE's `settings` up front. This will control the default directories for things like:\n",
    "\n",
    "- Data downloads with `seqdatasets`\n",
    "- Model configuration files (i.e. EUGENe will know where to look for these files without you having to specify the full path every time)\n",
    "- Model logs, checkpoints, and predictions\n",
    "- Figures and plots\n",
    "\n",
    "These small quality of life features can go a long way to preserve your sanity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to where you would like to save all your results\n",
    "import os\n",
    "os.chdir(\"/cellar/users/aklie/projects/ML4GLand/tutorials\")  # TODO: change this to your own directory\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:32:01.927249Z",
     "iopub.status.busy": "2022-10-15T21:32:01.926533Z",
     "iopub.status.idle": "2022-10-15T21:32:04.081680Z",
     "shell.execute_reply": "2022-10-15T21:32:04.081166Z",
     "shell.execute_reply.started": "2022-10-15T21:32:01.927228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure EUGENe directories, if you do not set these, EUGENe will use the default directories\n",
    "from eugene import settings\n",
    "settings.config_dir = \"./configs\" # Directory to specify when you want to load a model from a config file\n",
    "settings.dataset_dir = \"./datasets\" # Directory where EUGENe will download datasets to\n",
    "settings.logging_dir = \"./logs\" # Directory where EUGENe will save Tensorboard training logs and model checkpoints to\n",
    "settings.output_dir = \"./output\" # Directory where EUGENe will save output files to\n",
    "settings.figure_dir = \"./figures\" # Directory to specify to EUGENe to save figures to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "For this tutorial, we will reproduce the prediction of promoter activity featured in [Jores et al., 2021](https://www.nature.com/articles/s41477-021-00932-y) that uses DNA sequences as input to predict [STARR-seq activity](https://en.wikipedia.org/wiki/STARR-seq). We first need to load this dataset. If the dataset is a \"EUGENe benchmarking dataset\", it can be loaded in through the `SeqDatasets` [subpackage](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#seqdatasets-access-to-common-datasets). Let's load the package first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqdatasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next use the `get_dataset_info()` function to get information about the datasets available as \"EUGENe benchmarking datasets\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:35:41.534887Z",
     "iopub.status.busy": "2022-10-15T21:35:41.534578Z",
     "iopub.status.idle": "2022-10-15T21:35:43.803612Z",
     "shell.execute_reply": "2022-10-15T21:35:43.803127Z",
     "shell.execute_reply.started": "2022-10-15T21:35:41.534869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the dataset\n",
    "seqdatasets.get_dataset_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in luck! The plant promoter dataset is available via the `jores21()` command. If you are requesting this dataset for the for the first time, it will be downloaded and loaded into a `SeqData` object automagically (and downloaded to your `settings.dataset_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:35:57.432930Z",
     "iopub.status.busy": "2022-10-15T21:35:57.432530Z",
     "iopub.status.idle": "2022-10-15T21:36:01.948035Z",
     "shell.execute_reply": "2022-10-15T21:36:01.947507Z",
     "shell.execute_reply.started": "2022-10-15T21:35:57.432908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset to the dataset dir. We are using the promoters assayed in leaf promoters here\n",
    "sdata = seqdatasets.jores21(dataset=\"leaf\")\n",
    "sdata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about how you can use EUGENe to read from your standard genomics file formats or how we represents datasets in memory and on disk, check out the `SeqData` section of the [usage principles](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#seqdata) and the `SeqData` subpackage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Data visualization is a key part of the EUGENe workflow. We can use the `plot` module to visualize aspects of our data such as the distribution of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import plot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:18.433185Z",
     "iopub.status.busy": "2022-10-15T21:36:18.432767Z",
     "iopub.status.idle": "2022-10-15T21:36:21.763075Z",
     "shell.execute_reply": "2022-10-15T21:36:21.762600Z",
     "shell.execute_reply.started": "2022-10-15T21:36:18.433165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of targets across the different species the promoters were derived from\n",
    "pl.violinplot(sdata, vars=[\"enrichment\"], groupby=\"sp\", figsize=(4, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now that we have our data loaded in, we can preprocess it using EUGENe's `preprocess` module. This module wraps functionality from the `SeqPro` [subpackage](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#sequence-manipulation-is-handled-efficiently-by-seqpro) and includes several functions for common sequence preprocessing tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence preprocessing\n",
    "Our first preprocessing step will be to one-hot encode our sequences. One-hot encoding is a common way to represent sequences as a matrix of 0s and 1s for model training. We will pass in DNA as the vocab explicitly, but this is the default if not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import preprocess as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:22.736287Z",
     "iopub.status.busy": "2022-10-15T21:36:22.735818Z",
     "iopub.status.idle": "2022-10-15T21:36:28.594407Z",
     "shell.execute_reply": "2022-10-15T21:36:28.593906Z",
     "shell.execute_reply.started": "2022-10-15T21:36:22.736266Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One hot encode all the sequences in the sdata using the wrapper function\n",
    "pp.ohe_seqs_sdata(sdata, alphabet=\"DNA\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also quickly set-up some identifiers for our sequences. This will come in handy during interpretation when we often want to visualize aspects of specific sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make unique ids for each sequence in the sdata\n",
    "pp.make_unique_ids_sdata(sdata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "As is standard machine learning practice, we also need to split our data into training, validation, and test sets. This dataset comes with 'train' and 'test' labels in the 'set' annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:28.595485Z",
     "iopub.status.busy": "2022-10-15T21:36:28.595217Z",
     "iopub.status.idle": "2022-10-15T21:36:31.168534Z",
     "shell.execute_reply": "2022-10-15T21:36:31.168095Z",
     "shell.execute_reply.started": "2022-10-15T21:36:28.595468Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "sdata_train = sdata.sel(_sequence=(sdata[\"set\"] == \"train\").compute())\n",
    "sdata_test = sdata.sel(_sequence=(sdata[\"set\"] == \"test\").compute())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to split the training sequences into train and validation sets. We can do this using EUGENe's `train_test_random_split` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:31.169559Z",
     "iopub.status.busy": "2022-10-15T21:36:31.169407Z",
     "iopub.status.idle": "2022-10-15T21:36:33.794872Z",
     "shell.execute_reply": "2022-10-15T21:36:33.794379Z",
     "shell.execute_reply.started": "2022-10-15T21:36:31.169544Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the training set into training and validation\n",
    "pp.train_test_random_split(sdata_train, dim=\"_sequence\", train_var=\"train_val\", test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:33.795700Z",
     "iopub.status.busy": "2022-10-15T21:36:33.795515Z",
     "iopub.status.idle": "2022-10-15T21:36:36.707970Z",
     "shell.execute_reply": "2022-10-15T21:36:36.707489Z",
     "shell.execute_reply.started": "2022-10-15T21:36:33.795684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the split with a count plot\n",
    "pl.countplot(sdata_train, vars=\"train_val\", orient=\"h\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we have our data ready, it's time to train our model! Training in EUGENe is done through the [PyTorch Lightning (PL) framework](https://www.pytorchlightning.ai/index.html). However PyTorch Lightning does not offer us much help with instantiating model architectures and initializing them. We will utilize EUGENE's library of neural network parts and architectures to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation and initialization\n",
    "We first need to instantiate and initialize our model. We can use the `models` [module](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#models-instantiate-and-initialize-neural-network-architectures) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EUGENe offers several options for instantiating a model architecture. Here we will load in a Hybrid architecture that contains convoultional blocks that feed into recurrent layers, finishing with fully connected ones. We have set up a configuration file that trains pretty well on this dataset that you can download from [here](https://github.com/ML4GLand/tutorials/blob/main/configs/hybrid.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run the following to get the hybrid config downloaded\n",
    "#!mkdir -p $cwd/configs\n",
    "#!wget https://raw.githubusercontent.com/adamklie/EUGENe_paper/revision/configs/jores21/hybrid.yaml -O $cwd/configs/hybrid.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `load_config` function to load in this configuration file and initialize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_config(\"hybrid.yaml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out a summary of the model architecture using the `summary` function. Note that the configuration file we read in here also defines the LightningModule from PL that will be used to train the model. For more details on how this works, check out the [tutorial on instantiating and initializing models](https://github.com/ML4GLand/tutorials/blob/main/eugene/models/instantiating_models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:42.433746Z",
     "iopub.status.busy": "2022-10-15T21:36:42.433340Z",
     "iopub.status.idle": "2022-10-15T21:36:45.067028Z",
     "shell.execute_reply": "2022-10-15T21:36:45.066505Z",
     "shell.execute_reply.started": "2022-10-15T21:36:42.433726Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:36:45.068255Z",
     "iopub.status.busy": "2022-10-15T21:36:45.067937Z",
     "iopub.status.idle": "2022-10-15T21:36:47.834216Z",
     "shell.execute_reply": "2022-10-15T21:36:47.833737Z",
     "shell.execute_reply.started": "2022-10-15T21:36:45.068238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the weights\n",
    "models.init_weights(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a model intantiated and initialized, we are set up to fit our model to our plant promoters! We can do this through the `train` [module](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#train-fit-parameters-to-your-data) in EUGENe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using GPU accelerators on your machine, you can can use the `gpus` argument to set the number gpus you want to use. If left empty, EUGENe will try to infer the number of GPUs available. Training the model with a single GPU will take less than 5 minutes. Check out the API and docstring for the function below for more details on the arguments you can pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fit_sequence_module(\n",
    "    model=model,\n",
    "    sdata=sdata_train,\n",
    "    seq_var=\"ohe_seq\",\n",
    "    target_vars=[\"enrichment\"],\n",
    "    in_memory=True,\n",
    "    train_var=\"train_val\",\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    drop_last=False,\n",
    "    name=\"hybrid\",\n",
    "    version=\"tutorial_model\",\n",
    "    transforms={\"ohe_seq\": lambda x: x.swapaxes(1, 2)}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our models trained over time by plotting a training summary. All you need to do is point the [training_summary](https://eugene-tools.readthedocs.io/en/latest/api/eugene.plot.training_summary.html#eugene.plot.training_summary) function to your the EUGENe logging directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:41:11.432782Z",
     "iopub.status.busy": "2022-10-15T21:41:11.432554Z",
     "iopub.status.idle": "2022-10-15T21:41:15.367344Z",
     "shell.execute_reply": "2022-10-15T21:41:15.366919Z",
     "shell.execute_reply.started": "2022-10-15T21:41:11.432765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot a loss curve and an r2 curve as a metric\n",
    "pl.training_summary(os.path.join(settings.logging_dir, \"hybrid\", \"tutorial_model\"), metric=\"r2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After the model's been trained, we can evaluate our performance on our held-out test data. This is done through the `evaluate` [module](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#evaluate-validate-and-explore-models-on-new-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use our best model for evaluation. We can see from the training curve above that our model began overfitting the data after about 3000 training steps. Lucky for us, PyTorch Lightning keeps track of our best model for us! We can load this model in from the log directory like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the glob Python library to help us find the path to our model\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:41:19.432671Z",
     "iopub.status.busy": "2022-10-15T21:41:19.432293Z",
     "iopub.status.idle": "2022-10-15T21:41:22.356067Z",
     "shell.execute_reply": "2022-10-15T21:41:22.355560Z",
     "shell.execute_reply.started": "2022-10-15T21:41:19.432653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We point to the checkpoints directory within the logging directory to grab the best model\n",
    "model_file = glob.glob(os.path.join(settings.logging_dir, \"hybrid\", \"tutorial_model\", \"checkpoints\", \"*\"))[0]\n",
    "best_model = models.SequenceModule.load_from_checkpoint(model_file, arch=model.arch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is loaded in. Now let's make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:41:23.177586Z",
     "iopub.status.busy": "2022-10-15T21:41:23.177291Z",
     "iopub.status.idle": "2022-10-15T21:41:26.361478Z",
     "shell.execute_reply": "2022-10-15T21:41:26.361035Z",
     "shell.execute_reply.started": "2022-10-15T21:41:23.177570Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this best model to predict on the held-out data. This will store predictions in\n",
    "evaluate.predictions_sequence_module(\n",
    "    best_model,\n",
    "    sdata=sdata_test,\n",
    "    seq_var=\"ohe_seq\",\n",
    "    target_vars=\"enrichment\",\n",
    "    batch_size=2048,\n",
    "    in_memory=True,\n",
    "    name=\"hybrid\",\n",
    "    version=\"tutorial_model\",\n",
    "    file_label=\"test\",\n",
    "    prefix=f\"tutorial_model_\",\n",
    "    transforms={\"ohe_seq\": lambda x: x.swapaxes(1, 2)}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, these predictions are automatically stored in the SeqData object. We now have predictions from our trained model! Let's look at a scatterplot to see how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:41:34.671884Z",
     "iopub.status.busy": "2022-10-15T21:41:34.671572Z",
     "iopub.status.idle": "2022-10-15T21:41:37.680758Z",
     "shell.execute_reply": "2022-10-15T21:41:37.680317Z",
     "shell.execute_reply.started": "2022-10-15T21:41:34.671866Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.performance_scatter(\n",
    "    sdata_test, \n",
    "    target_vars=\"enrichment\", \n",
    "    prediction_vars=\"tutorial_model_enrichment_predictions\",\n",
    "    alpha=0.5,\n",
    "    groupby=\"sp\",\n",
    "    figsize=(8, 8)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby. We were able to train a pretty predictive model on this dataset with just DNA sequences as input!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "Seeing good model performance is a big step in the right direction, but is far from the whole picture. We also want to try to better understand what our model is learning. We can do this through model interpretation. Model interpretation in EUGENe is done through the `interpret` [module](https://eugene-tools.readthedocs.io/en/latest/usage-principles.html#seqexplainer-investigate-learned-model-behavior) which relies heavily on functionality built into the `SeqExplainer` [subpackage](https://github.com/ML4GLand/SeqExplainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import interpret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter interpretation\n",
    "We will first get an idea of what each filter of first convoulional layer of the model is seeing by using the `interpret` module's [`generate_pfms_sdata`](https://eugene-tools.readthedocs.io/en/latest/api/eugene.interpret.generate_pfms_sdata.html?highlight=generate_pfms#eugene.interpret.generate_pfms_sdata) function. This creates a position frequency matrix for each filter in the model using sequences that highly activate that filter (can be defined in multiple ways). We often times pass the the test sequences through the model, but you can theoretically pass any sequences you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.generate_pfms_sdata(\n",
    "    best_model,\n",
    "    sdata_test,\n",
    "    seq_var=\"ohe_seq\",\n",
    "    layer_name=\"arch.conv1d_tower.layers.1\",\n",
    "    kernel_size=13,\n",
    "    num_filters=256,\n",
    "    num_seqlets=100,\n",
    "    transforms={\"ohe_seq\": lambda x: x.swapaxes(1, 2)}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize a few of these PFMs to see if we can decipher what the filters are picking up on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize these PFMs as PWM logos\n",
    "pl.multifilter_viz(\n",
    "    sdata_test,\n",
    "    filter_nums=range(0, 32),\n",
    "    pfms_var=\"arch.conv1d_tower.layers.1_pfms\",\n",
    "    num_rows=8,\n",
    "    num_cols=4,\n",
    "    titles=[f\"filter {i}\" for i in range(0, 32)],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The qualitative visualization is nice, but often times we want to put some numbers behind what we are seeing. This is often done by annotating them PFMs from these filters against a database of known motifs with tools like [TomTom](https://meme-suite.org/meme/tools/tomtom). We offer a function for saving filters in an SeqData object to the MEME file format that can uploaded to the TomTom webtool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.filters_to_meme_sdata(\n",
    "    sdata_test,\n",
    "    filters_var=\"arch.conv1d_tower.layers.1_pfms\",\n",
    "    axis_order=(\"_arch.conv1d_tower.layers.1_256_filters\", \"_ohe\", \"_arch.conv1d_tower.layers.1_13_kernel_size\"),\n",
    "    output_dir=os.path.join(settings.output_dir),\n",
    "    filename=\"tutorial_model_best_model_filters.meme\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> Filters are not always interpretable to a human eye, or for that matter, to TomTom and databases of known motifs. The methods in this analysis are far from perfect, but can be a useful starting point for understanding what your model is learning. For more details on filter interpretation, check out the [filter interpretation tutorial](https://github.com/ML4GLand/tutorials/blob/main/seqexplainer/filter_interpretation.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribution analysis\n",
    "We will next perform an analysis in which we quantify the contribution of each nucleotide of an input sequence to the model's predictions for that sequence. This is called attribution analysis and can be performed in EUGENe with the `attribute_sdata` function. Here we will aply the [DeepLIFT](https://github.com/kundajelab/deeplift) method to our best model on our held-out test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.attribute_sdata(\n",
    "    best_model,\n",
    "    sdata_test,\n",
    "    method=\"DeepLift\",\n",
    "    batch_size=128,\n",
    "    reference_type=\"zero\",\n",
    "    transforms={\"ohe_seq\": lambda x: x.swapaxes(1, 2)}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize these importance scores using a sequence logo plot. Here we show the results on the sequences with the highest predicted activity by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:42:06.084905Z",
     "iopub.status.busy": "2022-10-15T21:42:06.084528Z",
     "iopub.status.idle": "2022-10-15T21:42:12.677194Z",
     "shell.execute_reply": "2022-10-15T21:42:12.676702Z",
     "shell.execute_reply.started": "2022-10-15T21:42:06.084883Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the top3 in terms of predictions to plot tracks for\n",
    "top3 = sdata_test[\"tutorial_model_enrichment_predictions\"].to_series().sort_values(ascending=False).iloc[:3].index\n",
    "ids = sdata_test[\"id\"].values[top3]\n",
    "pl.multiseq_track(\n",
    "    sdata_test,\n",
    "    seq_ids=ids,\n",
    "    attrs_vars = \"DeepLift_attrs\",\n",
    "    ylabs=\"DeepLift\",\n",
    "    height=3,\n",
    "    width=40,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> There are many nuances to attribution analysis that we won't get into here. For more details on how this works, check out the [tutorial on attribution analysis](https://github.com/ML4GLand/tutorials/blob/main/seqexplainer/attribution_analysis.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global importance analysis (GIA)\n",
    "Another class of interpretation methods that are gaining a lot of traction in the field are those that involve designing experiments for the model *in silico*. The general idea is see what model predicts when we feed it sequences we design ourselves and compare that to some baseline set of predictions. There are no shortage of potential ideas of what sequence to feed the model, but test the positional importance of a TATA box motif. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need some background sequences to establish baseline predictions. Here we use the SeqPro subpackage to generate 5 random sequences and make our own SeqData object from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import seqpro as sp\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SeqData object so its compatible with the function\n",
    "random_ohe_seq = sp.ohe(sp.random_seqs((5, 170), sp.alphabets.DNA), sp.alphabets.DNA).swapaxes(1, 2)\n",
    "sdata_random = xr.Dataset({\"ohe_seq\": ((\"_sequence\", \"_ohe\", \"length\"), random_ohe_seq)})\n",
    "pp.make_unique_ids_sdata(sdata_random, id_var=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our background predictions\n",
    "sdata_random[\"background_predictions\"] = best_model.predict(sdata_random[\"ohe_seq\"].values).squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the motif, we will use the MotifData subpackage in EUGENe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import motifdata as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The motif can be downlaoded from https://github.com/tobjores/Synthetic-Promoter-Designs-Enabled-by-a-Comprehensive-Analysis-of-Plant-Core-Promoters/blob/main/data/misc\n",
    "# !mkdir -p $cwd/data\n",
    "# !wget https://raw.githubusercontent.com/tobjores/Synthetic-Promoter-Designs-Enabled-by-a-Comprehensive-Analysis-of-Plant-Core-Promoters/main/data/misc/CPEs.meme -O $cwd/data/CPEs.meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load it and pull out the PFM and other info about the motif\n",
    "meme = md.read_meme(\"data/CPEs.meme\")\n",
    "motif = meme.motifs[\"TATA\"]\n",
    "feat_name = motif.name\n",
    "pfm = motif.pfm\n",
    "consensus = motif.consensus\n",
    "consensus_ohe = sp.ohe(consensus, alphabet=sp.alphabets.DNA)\n",
    "feat_name, pfm, consensus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a feature, we can implant it at every possible position of the input sequence and see what that does to model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the EUGENe function that does exactly that!\n",
    "interpret.positional_gia_sdata(\n",
    "    model=best_model, \n",
    "    sdata=sdata_random, \n",
    "    feature=consensus_ohe,\n",
    "    id_var=\"name\",\n",
    "    store_var=f\"slide_{feat_name}\",\n",
    "    encoding=\"onehot\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can visualize the results as a line plot using a custom function from the `plot` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pl.positional_gia_plot(sdata_random, vars=[f\"slide_{feat_name}\"], id_var=\"name\", return_axes=True)\n",
    "ax.hlines(sdata_random[\"background_predictions\"].mean(), 0, 170, linestyle=\"--\", color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence generation\n",
    "The last class of interpretability methods currently offered in EUGENe uses trained models to guide sequence evolution. We implement the simplest form of this approach that iteratively evolves a sequence by greedily inserting the mutation with the largest predicted impact at each iteration. Starting with an initial sequence (e.g. random, shuffled, etc.), this strategy can be used to evolve synthetic functional sequences.. This style of analysis is a promising direction for further research, and can also serve as an extension of ISM for validating that the model has learned representations that resemble motifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:43:26.847546Z",
     "iopub.status.busy": "2022-10-15T21:43:26.847354Z",
     "iopub.status.idle": "2022-10-15T21:43:33.810987Z",
     "shell.execute_reply": "2022-10-15T21:43:33.810501Z",
     "shell.execute_reply.started": "2022-10-15T21:43:26.847529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evolve this sequence for ten rounds\n",
    "interpret.evolve_seqs_sdata(model=best_model, sdata=sdata_random, rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the vars that start with \"evolved\"\n",
    "evolved_vars = [\"original_score\"] + [var for var in sdata_random.data_vars if var.startswith(\"evolved\") and var.endswith(\"score\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T21:43:34.588870Z",
     "iopub.status.busy": "2022-10-15T21:43:34.588634Z",
     "iopub.status.idle": "2022-10-15T21:43:36.644133Z",
     "shell.execute_reply": "2022-10-15T21:43:36.643651Z",
     "shell.execute_reply.started": "2022-10-15T21:43:34.588852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the predicted value at each round of evolution\n",
    "sdata_random[evolved_vars].to_dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "That concludes our basic usage tutorial! We hope you found it helpful. Don't hesitate to raise a GitHub issue if you run into any errors or if anything is overly confusing!\n",
    "\n",
    "You can find more tutorials dedicated to many of the specific steps shown here on the ML4GLand tutorials repo (https://github.com/ML4GLand/tutorials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
